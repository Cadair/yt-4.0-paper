## Processing and Analysis of Data

`yt` provides several interfaces for accessing the data available in a given dataset.
As described in @sec:data_objects, the primary means of accessing data is through "data objects" that apply selections to the dataset.
These objects present dictionary-like interfaces that return data; below, we describe what options are available for the data that is returned (@sec:field_system), as well as high-level interfaces for applying aggregations and reductions (@sec:arraylike_operations).

### Field System {#sec:field_system}

In `yt`, there are three types of "fields" that define values at a given spatial location.
The first of these is an "on-disk" field, representing the raw, unmodified (except potentially up-cast to 64 bit precision) values read from the data storage that defines the dataset, such as files or bucket storage; while `yt` does provide routines for reading these fields, they are passed largely unmodified and so we do not discuss them in depth.
The second type of field is a "derived field," which is a functional definition of how to process or combine one or more fields that exist in the dataset.
Finally, providing the closure necessary for these derived fields to be accessed independently of their naming convention are "alias fields" that provide mappings between platform- or format-specific names for fields and those used internally in `yt`.
This collection of three classes of fields

Fields are also defined by their "sampling type" to distinguish between those fields defined in a volume-filling fashion (i.e., cell-based fields) and those that are defined by discrete samples that may or may not require closure or convolution functions to be applied.
Fields that are defined as a collection of discrete samples can be combined or filtered differently than those that are defined in a volume-filling manner, as described in @secparticle_filters and @sec:particle_unions.

#### Field Aliases {#sec:field_aliases}

Small differences in naming fields can prove disproportionately challenging for writing platform-neutral analysis code.
For instance, if one platform names the "density" field `dens` and another refers to it as `Density` (or, as we have seen in one platform, even the unicode character for $\rho$) then any platform-independent derived field that utilizes density must be defined multiple times to refer to this fundamentally identical quantity.
(An important note here is that in many cases, the reverse problem is true -- some codes may refer to things with the same name but with different underlying definitions, which provides an additional challenge to the analysis process by requiring [disambiguation](https://www.youtube.com/watch?v=WZLkcFns8Ks).)

To address this issue, `yt` defines a set of fundamental fields, along with a naming convention for extensibility, that are provided as "aliases" for the dataset-specific field names.
This enables a consistent ontology to be defined for fields in `yt`, upon which the remainder of derived fields can rely.
Typically these are defined by the authors of a given dataset format frontend, wherein a translation or lookup table is provided to match the on-disk fields to those expected by `yt`.

In some cases, it is through a combination of derived and aliased fields that the full set of data is made available to the researcher; for instance, some datasets do not store velocity as a quantity on disk, but instead store momentum.
In this case, momentum is aliased from the on-disk field to the `yt` field, and then a derived field is generated to seamlessly provide access to the velocity field wherever it is needed.

#### Derived Fields {#sec:derived_fields}

#### Particle Filters {#sec:particle_filters}

#### Particle Unions {#sec:particle_unions}

#### Field Detection {#sec:field_detection}

Determining ahead-of-time the fields that are able to be computed 

### Array-like Operations {#sec:arraylike_operations}

In `yt`, a newly-constructed data selector contains no data -- this enables data selectors for large regions, in extremely large datasets, to be lightweight and cheap to construct.
By ensuring that these objects don't immediately consume resources, they can be manipulated and operated on in a high-level fashion, without taxing the computational power.
While these data objects *can* return the full set of data they include, `yt` also provides array-like operations that do not require immediate access to the full set of numerical values, and which align with the mental-model for data processing that `yt` exposes.
As an example, consider the following two operations:

```python
dd = ds.all_data()
dd["gas", "density"].max()
```

and

```python
dd = ds.all_data()
dd.max(("gas", "density"))
```

Both are available in `yt`.
As a side-effect of Python's object model, the first will access the `("gas", "density")` item in the object `dd`, itself a concatenated numpy array, and then execute the `max` method on it.
The second will call the `max` method on the data object, supplying to it the name of the field.
This allows `yt` to decide how to decompose, parallelize and process the data in a memory-efficient way, and spread across multiple processors.
Additionally, by emphasizing that the "maximum" is being taken on the data object, rather than the numerical data, other operations can be exposed that build on the underlying data organization.
For instance, taking the maximum along a given (spatial) axis:

```python
sp = ds.sphere("center", (10.0, "m"))
sp.max(("gas", "temperature"), axis="z")
```

This translates our meaning -- find the maximum value along the z-axis -- into a dimensionality reduction operation that uses `yt`'s built-in "projection" method.
These operations, on data objects (rather than the underlying arrays of values that are accessible through them) provide dataframe-like methods for querying very large, spatially registered data.

The array-like operations utilized in `yt` attempt to map to conceptually similar operations in numpy.
Unlike numpy, however, these utilize `yt`'s dataset-aware "chunking" operations, in a manner philosophically similar to the chunking operations used in dask.
Below, we outline the three classes of operations that are available, based on the type of their return value.

#### Reduction to Scalars  {#sec:arrayops-scalar}

Traditional array operations that map from an array to a scalar are accessible utilizing familiar syntax.  These include:

 * `min(field_specification)`, `max(field_specification)`, and `ptp(field_specification)`
 * `argmin(field_specification, axis)`, and `argmax(field_specification, axis)`
 * `mean(field_specification, weight)`, `std(field_specification, weight)`, and `sum(field_specification)`

In addition to the advantages of allowing the parallelism and memory management be handled by `yt`, these operations are also able to accept multiple fields.
This allows multiple fields to be queried in a single pass over the data, rather than multiple passes.
Additionally, the `min` and `max` operations will automatically cache the results during a single pass, which means that calling `max` immediately after `min` (and vice versa) on the same data object and field will not require a recomputation.

In the case of `argmin` and `argmax`, the default returned "axis" will be the spatial coordinates of the minimum or maximum field value (respectively).  
However, by specifying an axis or set of axes that correspond to fields, the field values will be queried at these minimum or maximum points.
This allows, for instance, to query the value of "density" at the minimum "temperature."
The operations `mean` and `sum` are available here in a non-spatial form, where they simply compute the scalar reduction independent of the spatial registration of the dataset.

#### Reduction to Vectors {#sec:arrayops-vector}

 * `profile(axes, fields, profile_specification)`

The `profile` operation provides weighted or unweighted histogramming in one or two dimensions.
This function accepts the axes along which to compute the histogram as well as the fields to compute, and information about whether the binning should be an accumulation, an average, or a weighted average.
These operations are described in more detail in **reference profile section**.

#### Remapping Operations {#sec:arrayops-remap}

 * `mean(field_specification, weight, axis)`
 * `sum(field_specification, axis)`
 * `integrate(field_specification, weight, axis)`

These functions map directly to different methods used by the projection data object.
Both `mean` and `sum`, when supplied a spatial axis, will compute a dimensionally-reduced projection, remapped into a pixel coordinate plane.
Importantly, if the dataset is a finite-volume dataset (grid, octree, etc), the results of these operations will be a variable-resolution mesh, rather than a fixed resolution image buffer.
